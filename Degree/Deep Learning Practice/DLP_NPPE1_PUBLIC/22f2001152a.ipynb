{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":81881,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102},{"sourceId":104449,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T04:22:09.774559Z","iopub.execute_input":"2025-02-18T04:22:09.774832Z","iopub.status.idle":"2025-02-18T04:22:11.097778Z","shell.execute_reply.started":"2025-02-18T04:22:09.774805Z","shell.execute_reply":"2025-02-18T04:22:11.096873Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3.1/transformers/8b-instruct/1/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\n/kaggle/input/multi-lingual-sentiment-analysis/train.csv\n/kaggle/input/multi-lingual-sentiment-analysis/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install bitsandbytes\n!pip install unsloth\n!pip install accelerate\n!pip install peft\n!pip install torch==2.1.2 --force-reinstall\n!pip install --upgrade transformers\n!pip install --upgrade unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:10:24.270891Z","iopub.execute_input":"2025-02-17T06:10:24.271182Z","iopub.status.idle":"2025-02-17T06:17:05.125333Z","shell.execute_reply.started":"2025-02-17T06:10:24.271152Z","shell.execute_reply":"2025-02-17T06:17:05.124383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch._dynamo\nprint(torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:17:05.127152Z","iopub.execute_input":"2025-02-17T06:17:05.127522Z","iopub.status.idle":"2025-02-17T06:17:08.007003Z","shell.execute_reply.started":"2025-02-17T06:17:05.127463Z","shell.execute_reply":"2025-02-17T06:17:08.006304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\nfrom unsloth import FastLanguageModel\nimport torch\nimport pandas as pd\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:17:08.008122Z","iopub.execute_input":"2025-02-17T06:17:08.008506Z","iopub.status.idle":"2025-02-17T06:17:32.871913Z","shell.execute_reply.started":"2025-02-17T06:17:08.008483Z","shell.execute_reply":"2025-02-17T06:17:32.870992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\ntest_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\"\nmodel_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\nRANDOM_SEED=1971","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:17:32.872863Z","iopub.execute_input":"2025-02-17T06:17:32.873174Z","iopub.status.idle":"2025-02-17T06:17:32.877029Z","shell.execute_reply.started":"2025-02-17T06:17:32.873143Z","shell.execute_reply":"2025-02-17T06:17:32.876231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nlang_map = {\n    \"as\": \"Assamese\",\n    \"bd\": \"Bodo\",\n    \"bn\": \"Bengali\",\n    \"gu\": \"Gujarati\",\n    \"hi\": \"Hindi\",\n    \"kn\": \"Kannada\",\n    \"ml\": \"Malayalam\",\n    \"mr\": \"Marathi\",\n    \"or\": \"Odia\",\n    \"pa\": \"Punjabi\",\n    \"ta\": \"Tamil\",\n    \"te\": \"Telugu\",\n    \"ur\": \"Urdu\"\n}\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_path,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:17:32.878001Z","iopub.execute_input":"2025-02-17T06:17:32.878318Z","iopub.status.idle":"2025-02-17T06:18:52.020698Z","shell.execute_reply.started":"2025-02-17T06:17:32.878286Z","shell.execute_reply":"2025-02-17T06:18:52.020049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = RANDOM_SEED,  #3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:18:52.022659Z","iopub.execute_input":"2025-02-17T06:18:52.022888Z","iopub.status.idle":"2025-02-17T06:18:57.89769Z","shell.execute_reply.started":"2025-02-17T06:18:52.022868Z","shell.execute_reply":"2025-02-17T06:18:57.897013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nPredict the sentiment of {} language sentence as 1 (positive) or 0 (negative), output 0 or 1.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    texts = []\n    num_labels = []\n    sentences = examples['sentence']\n    languages = examples['language']\n    labels = examples['label']\n    for sentence, language, label in zip(sentences, languages, labels):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        label_01 = 1 if label == 'Positive' else 0\n        text = alpaca_prompt.format(lang_map[language], sentence, label_01) + EOS_TOKEN\n        texts.append(text)\n        num_labels.append(label_01)\n    return { \"text\" : texts, \"label\": num_labels}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:29:23.355088Z","iopub.execute_input":"2025-02-17T06:29:23.355425Z","iopub.status.idle":"2025-02-17T06:29:23.360997Z","shell.execute_reply.started":"2025-02-17T06:29:23.355399Z","shell.execute_reply":"2025-02-17T06:29:23.360077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"csv\", data_files = [train_file_path])[\"train\"]\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n\nsplit_dataset = dataset.train_test_split(test_size = 0.2, seed = 62)\ntrain_dataset, valid_dataset = split_dataset['train'], split_dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:29:25.479014Z","iopub.execute_input":"2025-02-17T06:29:25.47934Z","iopub.status.idle":"2025-02-17T06:29:25.617118Z","shell.execute_reply.started":"2025-02-17T06:29:25.479313Z","shell.execute_reply":"2025-02-17T06:29:25.616449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset, valid_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:29:30.523069Z","iopub.execute_input":"2025-02-17T06:29:30.523377Z","iopub.status.idle":"2025-02-17T06:29:30.528872Z","shell.execute_reply.started":"2025-02-17T06:29:30.523352Z","shell.execute_reply":"2025-02-17T06:29:30.528024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0], valid_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:29:36.169926Z","iopub.execute_input":"2025-02-17T06:29:36.170281Z","iopub.status.idle":"2025-02-17T06:29:36.177669Z","shell.execute_reply.started":"2025-02-17T06:29:36.17025Z","shell.execute_reply":"2025-02-17T06:29:36.176907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n        warmup_steps = 5,\n        num_train_epochs = 5, # Set this for 1 full training run.\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 10,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:29:45.507997Z","iopub.execute_input":"2025-02-17T06:29:45.508322Z","iopub.status.idle":"2025-02-17T06:29:50.219507Z","shell.execute_reply.started":"2025-02-17T06:29:45.508295Z","shell.execute_reply":"2025-02-17T06:29:50.218756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:30:05.093747Z","iopub.execute_input":"2025-02-17T06:30:05.094085Z","iopub.status.idle":"2025-02-17T07:55:01.525254Z","shell.execute_reply.started":"2025-02-17T06:30:05.094055Z","shell.execute_reply":"2025-02-17T07:55:01.524279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference Test\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\nexample = train_dataset[0]\ninputs = tokenizer(\n[\n    alpaca_prompt.format(lang_map[example['language']], example['sentence'], '')\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\nresult = tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:27:47.557083Z","iopub.execute_input":"2025-02-17T08:27:47.557452Z","iopub.status.idle":"2025-02-17T08:27:48.346128Z","shell.execute_reply.started":"2025-02-17T08:27:47.557424Z","shell.execute_reply":"2025-02-17T08:27:48.345344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result, example['label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:27:51.290883Z","iopub.execute_input":"2025-02-17T08:27:51.291234Z","iopub.status.idle":"2025-02-17T08:27:51.296271Z","shell.execute_reply.started":"2025-02-17T08:27:51.291177Z","shell.execute_reply":"2025-02-17T08:27:51.295546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\ndef apply_inference(examples):\n    results = []\n    sentences = examples['sentence']\n    languages = examples['language']\n    inputs = tokenizer(\n        [\n            alpaca_prompt.format(lang_map[languages], sentences, '')\n        ], return_tensors = \"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n    result = int(tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0])\n    return {'result': result}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:29:50.695695Z","iopub.execute_input":"2025-02-17T08:29:50.696041Z","iopub.status.idle":"2025-02-17T08:29:50.701585Z","shell.execute_reply.started":"2025-02-17T08:29:50.696012Z","shell.execute_reply":"2025-02-17T08:29:50.700547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.map(apply_inference)\nvalid_dataset = valid_dataset.map(apply_inference)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:29:53.333738Z","iopub.execute_input":"2025-02-17T08:29:53.334039Z","iopub.status.idle":"2025-02-17T08:39:38.264465Z","shell.execute_reply.started":"2025-02-17T08:29:53.334015Z","shell.execute_reply":"2025-02-17T08:39:38.263674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ComputeF1(actual, predicted):\n    tp, tn, fp, fn = 0, 0, 0, 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            if actual[i] == 1: tp += 1\n            else: tn += 1\n        else:\n            if actual[i] == 1: fn += 1\n            else: fp += 1\n    precision = tp / (tp+fp)\n    recall = tp / (tp+fn)\n    accuracy = (tp+tn) / (tp+tn+fp+fn)\n    f1_score = (2 * precision * recall) / (precision+recall)\n    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}')\n    print(f'F1 Score: {f1_score}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:15.919768Z","iopub.execute_input":"2025-02-17T08:40:15.920083Z","iopub.status.idle":"2025-02-17T08:40:15.92556Z","shell.execute_reply.started":"2025-02-17T08:40:15.920059Z","shell.execute_reply":"2025-02-17T08:40:15.924701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Training Data Set Performance')\nComputeF1(train_dataset[:]['label'], train_dataset[:]['result'])\nprint('Validation Data Set Performance')\nComputeF1(valid_dataset[:]['label'], valid_dataset[:]['result'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:20.442973Z","iopub.execute_input":"2025-02-17T08:40:20.44329Z","iopub.status.idle":"2025-02-17T08:40:20.627059Z","shell.execute_reply.started":"2025-02-17T08:40:20.443266Z","shell.execute_reply":"2025-02-17T08:40:20.62627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"lora_model_2048\") # Local saving\ntokenizer.save_pretrained(\"lora_model_2048\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:35.269415Z","iopub.execute_input":"2025-02-17T08:40:35.269768Z","iopub.status.idle":"2025-02-17T08:40:35.775558Z","shell.execute_reply.started":"2025-02-17T08:40:35.269743Z","shell.execute_reply":"2025-02-17T08:40:35.774642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = load_dataset(\"csv\", data_files = [test_file_path])[\"train\"]\ntest_dataset = test_dataset.map(get_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:39.240503Z","iopub.execute_input":"2025-02-17T08:40:39.240836Z","iopub.status.idle":"2025-02-17T08:41:51.515064Z","shell.execute_reply.started":"2025-02-17T08:40:39.240806Z","shell.execute_reply":"2025-02-17T08:41:51.51424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = test_dataset[:]['result']\npredictions = ['Positive' if predictions[i] == 1 else 'Negative' for i in range(len(predictions))]\nlen(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:43:46.649318Z","iopub.execute_input":"2025-02-17T08:43:46.649717Z","iopub.status.idle":"2025-02-17T08:43:46.818428Z","shell.execute_reply.started":"2025-02-17T08:43:46.649688Z","shell.execute_reply":"2025-02-17T08:43:46.817621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dl = pd.DataFrame({'ID': list(range(1,len(predictions)+1)), 'label': predictions})\ndl.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:43:49.078905Z","iopub.execute_input":"2025-02-17T08:43:49.079237Z","iopub.status.idle":"2025-02-17T08:43:49.092173Z","shell.execute_reply.started":"2025-02-17T08:43:49.079181Z","shell.execute_reply":"2025-02-17T08:43:49.0913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}