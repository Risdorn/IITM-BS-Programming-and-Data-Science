{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82aadc2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-16T11:24:52.867283Z",
     "iopub.status.busy": "2025-02-16T11:24:52.866932Z",
     "iopub.status.idle": "2025-02-16T11:24:53.604903Z",
     "shell.execute_reply": "2025-02-16T11:24:53.603890Z"
    },
    "papermill": {
     "duration": 0.746602,
     "end_time": "2025-02-16T11:24:53.606511",
     "exception": false,
     "start_time": "2025-02-16T11:24:52.859909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\n",
      "/kaggle/input/multi-lingual-sentiment-analysis/train.csv\n",
      "/kaggle/input/multi-lingual-sentiment-analysis/test.csv\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b1a959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:24:53.618318Z",
     "iopub.status.busy": "2025-02-16T11:24:53.617969Z",
     "iopub.status.idle": "2025-02-16T11:25:20.096892Z",
     "shell.execute_reply": "2025-02-16T11:25:20.095887Z"
    },
    "papermill": {
     "duration": 26.486479,
     "end_time": "2025-02-16T11:25:20.098735",
     "exception": false,
     "start_time": "2025-02-16T11:24:53.612256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.9.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.45.2\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.28.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.28.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.9.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\r\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.47.0\r\n",
      "    Uninstalling transformers-4.47.0:\r\n",
      "      Successfully uninstalled transformers-4.47.0\r\n",
      "Successfully installed transformers-4.48.3\r\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install peft\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592c9715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:25:20.115781Z",
     "iopub.status.busy": "2025-02-16T11:25:20.115513Z",
     "iopub.status.idle": "2025-02-16T11:25:43.282686Z",
     "shell.execute_reply": "2025-02-16T11:25:43.281787Z"
    },
    "papermill": {
     "duration": 23.177204,
     "end_time": "2025-02-16T11:25:43.284343",
     "exception": false,
     "start_time": "2025-02-16T11:25:20.107139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d487f11d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:25:43.301505Z",
     "iopub.status.busy": "2025-02-16T11:25:43.301001Z",
     "iopub.status.idle": "2025-02-16T11:27:01.671206Z",
     "shell.execute_reply": "2025-02-16T11:27:01.670346Z"
    },
    "papermill": {
     "duration": 78.380377,
     "end_time": "2025-02-16T11:27:01.672927",
     "exception": false,
     "start_time": "2025-02-16T11:25:43.292550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b533a80dbc3448b8365a807c3687f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "model_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n",
    "\n",
    "\n",
    "# Quantization configuration\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Loading the model and tokenizer\n",
    "\n",
    "modelCLM = AutoModelForCausalLM.from_pretrained(model_path,quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    model_max_length=1200,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273e630e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:01.689757Z",
     "iopub.status.busy": "2025-02-16T11:27:01.689492Z",
     "iopub.status.idle": "2025-02-16T11:27:04.750551Z",
     "shell.execute_reply": "2025-02-16T11:27:04.749603Z"
    },
    "papermill": {
     "duration": 3.071152,
     "end_time": "2025-02-16T11:27:04.752199",
     "exception": false,
     "start_time": "2025-02-16T11:27:01.681047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\r\n",
      "Version: 4.48.3\r\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\r\n",
      "Home-page: https://github.com/huggingface/transformers\r\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\r\n",
      "Author-email: transformers@huggingface.co\r\n",
      "License: Apache 2.0 License\r\n",
      "Location: /usr/local/lib/python3.10/dist-packages\r\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\r\n",
      "Required-by: kaggle-environments, peft, sentence-transformers\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314177d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T07:24:45.706567Z",
     "iopub.status.busy": "2025-02-16T07:24:45.706218Z"
    },
    "papermill": {
     "duration": 0.008059,
     "end_time": "2025-02-16T11:27:04.768730",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.760671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "modelCLS = AutoModelForSequenceClassification.from_pretrained(model_path,quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be47462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:20:06.234259Z",
     "iopub.status.busy": "2025-02-15T09:20:06.233977Z",
     "iopub.status.idle": "2025-02-15T09:20:06.239482Z",
     "shell.execute_reply": "2025-02-15T09:20:06.238728Z",
     "shell.execute_reply.started": "2025-02-15T09:20:06.234239Z"
    },
    "papermill": {
     "duration": 0.007228,
     "end_time": "2025-02-16T11:27:04.783694",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.776466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "classifier = TextClassificationPipeline(model=modelCLS,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       framework='pt',\n",
    "                                       task=\"sentiment-analysis\",\n",
    "                                       #device = \"cuda\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c40fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:04.800574Z",
     "iopub.status.busy": "2025-02-16T11:27:04.800290Z",
     "iopub.status.idle": "2025-02-16T11:27:04.803867Z",
     "shell.execute_reply": "2025-02-16T11:27:04.803241Z"
    },
    "papermill": {
     "duration": 0.01347,
     "end_time": "2025-02-16T11:27:04.805246",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.791776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75a410f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:04.821748Z",
     "iopub.status.busy": "2025-02-16T11:27:04.821490Z",
     "iopub.status.idle": "2025-02-16T11:27:04.862569Z",
     "shell.execute_reply": "2025-02-16T11:27:04.861698Z"
    },
    "papermill": {
     "duration": 0.050835,
     "end_time": "2025-02-16T11:27:04.863984",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.813149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path,index_col=0)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "print(f\"Test set shape: {test_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7fd4ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:04.880414Z",
     "iopub.status.busy": "2025-02-16T11:27:04.880210Z",
     "iopub.status.idle": "2025-02-16T11:27:04.913546Z",
     "shell.execute_reply": "2025-02-16T11:27:04.912815Z"
    },
    "papermill": {
     "duration": 0.042992,
     "end_time": "2025-02-16T11:27:04.914962",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.871970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path,index_col=0)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df)\n",
    "print(f\"Training set shape: {train_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8233eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:04.931488Z",
     "iopub.status.busy": "2025-02-16T11:27:04.931259Z",
     "iopub.status.idle": "2025-02-16T11:27:04.934274Z",
     "shell.execute_reply": "2025-02-16T11:27:04.933481Z"
    },
    "papermill": {
     "duration": 0.0126,
     "end_time": "2025-02-16T11:27:04.935586",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.922986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sub_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\"\n",
    "\n",
    "#dfSub = pd.read_csv(sub_file_path,index_col=0)\n",
    "#dfSub = pd.DataFrame(dfSub)\n",
    "\n",
    "#dfSub_dataset = Dataset.from_pandas(dfSub)\n",
    "#print(f\"Submission set shape: {dfSub_dataset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f002a02a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:04.952127Z",
     "iopub.status.busy": "2025-02-16T11:27:04.951928Z",
     "iopub.status.idle": "2025-02-16T11:27:04.954873Z",
     "shell.execute_reply": "2025-02-16T11:27:04.954162Z"
    },
    "papermill": {
     "duration": 0.012665,
     "end_time": "2025-02-16T11:27:04.956143",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.943478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dfSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93451a0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:04.972389Z",
     "iopub.status.busy": "2025-02-16T11:27:04.972182Z",
     "iopub.status.idle": "2025-02-16T11:27:04.975451Z",
     "shell.execute_reply": "2025-02-16T11:27:04.974713Z"
    },
    "papermill": {
     "duration": 0.012944,
     "end_time": "2025-02-16T11:27:04.976857",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.963913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "L_Positive = \"Positive\"\n",
    "L_Negative = \"Negative\"\n",
    "\n",
    "id2label = {0: L_Positive, 1: L_Negative }\n",
    "label_mapping = {\n",
    "    \"LABEL_0\": L_Positive,\n",
    "    \"LABEL_1\": L_Negative\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52739fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:04.994068Z",
     "iopub.status.busy": "2025-02-16T11:27:04.993841Z",
     "iopub.status.idle": "2025-02-16T11:27:04.998246Z",
     "shell.execute_reply": "2025-02-16T11:27:04.997426Z"
    },
    "papermill": {
     "duration": 0.014592,
     "end_time": "2025-02-16T11:27:04.999593",
     "exception": false,
     "start_time": "2025-02-16T11:27:04.985001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SACLS_Prompt(input):\n",
    "#    prompt = f'''Find sentiment of this {map_dict[input['language']]} Text: {input['sentence']}'''\n",
    "#    return prompt\n",
    "    return input\n",
    "    \n",
    "def BATCH_SACLS_Sentiment(dataset, classifier):\n",
    "    predList=[]\n",
    "    for i in range(dataCount):\n",
    "        prompt = SACLS_Prompt(dataset['sentence'][i])\n",
    "        predLabel = label_mapping.get(classifier(prompt)[0]['label'])\n",
    "        predList.append(predLabel)\n",
    "\n",
    "    return predList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dde059f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.016177Z",
     "iopub.status.busy": "2025-02-16T11:27:05.015906Z",
     "iopub.status.idle": "2025-02-16T11:27:05.020495Z",
     "shell.execute_reply": "2025-02-16T11:27:05.019637Z"
    },
    "papermill": {
     "duration": 0.014014,
     "end_time": "2025-02-16T11:27:05.021717",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.007703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CLS_Sentiment(prompt, model, tokenizer,max_length=1200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    id=torch.argmax(outputs.logits, 1)\n",
    "    label=id2label[int(id)]\n",
    "    score=torch.softmax(outputs.logits, 1)\n",
    "    score = float(max(score[0]))\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8651d7ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.038527Z",
     "iopub.status.busy": "2025-02-16T11:27:05.038312Z",
     "iopub.status.idle": "2025-02-16T11:27:05.041802Z",
     "shell.execute_reply": "2025-02-16T11:27:05.041097Z"
    },
    "papermill": {
     "duration": 0.013021,
     "end_time": "2025-02-16T11:27:05.043006",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.029985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_dict = {\n",
    "    \"as\": \"Assamese\",\n",
    "    \"bd\": \"Bodo\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"gu\": \"Gujarati\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"kn\": \"Kannada\",\n",
    "    \"ml\": \"Malayalam\",\n",
    "    \"mr\": \"Marathi\",\n",
    "    \"or\": \"Odia\",\n",
    "    \"pa\": \"Punjabi\",\n",
    "    \"ta\": \"Tamil\",\n",
    "    \"te\": \"Telugu\",\n",
    "    \"ur\": \"Urdu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35ae74",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-15T10:21:16.909386Z",
     "iopub.status.busy": "2025-02-15T10:21:16.909043Z",
     "iopub.status.idle": "2025-02-15T10:48:27.073536Z",
     "shell.execute_reply": "2025-02-15T10:48:27.072615Z",
     "shell.execute_reply.started": "2025-02-15T10:21:16.909363Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.007578,
     "end_time": "2025-02-16T11:27:05.058419",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.050841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "dataCount=1000\n",
    "count=0\n",
    "for i in range(dataCount):\n",
    "    text = train_dataset['sentence'][i]\n",
    "    refLabel = train_dataset['label'][i]\n",
    "    predLabel = label_mapping.get(classifier(text)[0]['label'])\n",
    "    if refLabel==predLabel:\n",
    "        count = count+1\n",
    "    if i%5==0:\n",
    "        print(i, count/(i+1))\n",
    "\n",
    "print(i, count/dataCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "231fa7bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.074764Z",
     "iopub.status.busy": "2025-02-16T11:27:05.074515Z",
     "iopub.status.idle": "2025-02-16T11:27:05.079488Z",
     "shell.execute_reply": "2025-02-16T11:27:05.078785Z"
    },
    "papermill": {
     "duration": 0.014584,
     "end_time": "2025-02-16T11:27:05.080942",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.066358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#    Determine if it is positive or negative, and return the answer as the corresponding sentiment label \"Positive\" or  \"Negative\".\n",
    "#    Give only one output. Stop after writing the predicted sentiment.\n",
    "#     Stop once you write '_SENTIMENT_ = Positive' or '_SENTIMENT_ = Negative'\n",
    "#    Give the answer in this format: '_SENTIMENT_ = <predicted-sentiment>'\n",
    "\n",
    "SENTIMENT_SEP = '_SENTIMENT_'\n",
    "\n",
    "def CLM_Prompt(input):\n",
    "    prompt1 = f'''\n",
    "    Analyse the sentiment of this {lang_dict[input['language']]} sentence enclosed in square brackets.\n",
    "    Determine if it is positive or negative, and return the answer as the corresponding sentiment label.\n",
    "    Do not use any programming method and use your domain knowledge.\n",
    "    No explanation. Do not provide any additional note.\n",
    "    \n",
    "    [ {input['sentence']} ]\n",
    "\n",
    "    label: \n",
    "    '''\n",
    "\n",
    "    prompt2 = f'''Predict the sentiment of the following sentence: {input['sentence']} as either positive or negative\n",
    "    \n",
    "    This sentence comes from the {lang_dict[input['language']]} language.\n",
    "    Please provide your answer  as per the given structure: `The predicted sentiment of the statement is <predicted-sentiment>.`\n",
    "    Try to avoid any programming method and use your domain knowledge\n",
    "    Don't generate any explanation or reasoning in support of your answer'''\n",
    "\n",
    "    prompt3=f'''\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a helpful assistant. Predict the sentiment of the given {lang_dict[input['language']]} language sentence as POSITIVE or NEGATIVE. <|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {input['sentence']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    '''\n",
    "\n",
    "    prompt4=f'''\n",
    "    Analyze the sentiment of the following text. Classify the sentiment as either \"Positive\" or \"Negative\" only. Do not classify it as Neutral or give any other responses. Return just the sentiment without any additional explanation. Do not use any programming language.\n",
    "\n",
    "    If you cannot determine the sentiment, translate the text to English and classify the sentiment again. Return just the sentiment without any additional explanation.\n",
    "\n",
    "    Text: \"{input['sentence']}\"\n",
    "\n",
    "    Sentiment:\n",
    "    '''\n",
    "\n",
    "    prompt5=f'''\n",
    "    Analyse the sentiment of  this {lang_dict[input['language']]} text and Classify it into positive or negative, and return the answer as the corresponding sentiment label.\n",
    "    text: {input[\"sentence\"]}\n",
    "    label: \n",
    "    '''\n",
    "    return prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6589ca25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.097189Z",
     "iopub.status.busy": "2025-02-16T11:27:05.096946Z",
     "iopub.status.idle": "2025-02-16T11:27:05.101394Z",
     "shell.execute_reply": "2025-02-16T11:27:05.100688Z"
    },
    "papermill": {
     "duration": 0.013716,
     "end_time": "2025-02-16T11:27:05.102504",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.088788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CLM_Sentiment(prompt, model, tokenizer, max_length=200):\n",
    "    # Tokenize input prompt and move to the correct device\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        pred = model.generate(**tokens, \n",
    "                              max_new_tokens=2,\n",
    "                              temperature=0.1)\n",
    "\n",
    "    # Decode and return generated text\n",
    "    return tokenizer.decode(pred[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b00c9d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.118537Z",
     "iopub.status.busy": "2025-02-16T11:27:05.118340Z",
     "iopub.status.idle": "2025-02-16T11:27:05.121446Z",
     "shell.execute_reply": "2025-02-16T11:27:05.120791Z"
    },
    "papermill": {
     "duration": 0.012237,
     "end_time": "2025-02-16T11:27:05.122549",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.110312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loPositive=\"positive\"\n",
    "loNegative=\"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caa3ce04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.138648Z",
     "iopub.status.busy": "2025-02-16T11:27:05.138423Z",
     "iopub.status.idle": "2025-02-16T11:27:05.142250Z",
     "shell.execute_reply": "2025-02-16T11:27:05.141397Z"
    },
    "papermill": {
     "duration": 0.013007,
     "end_time": "2025-02-16T11:27:05.143443",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.130436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PostProcess(text):\n",
    "    text = text.lower()\n",
    "    #index = output.find(SENTIMENT_SEP)\n",
    "    countPositive = text.count(loPositive)\n",
    "    countNegative = text.count(loNegative)\n",
    "\n",
    "    if countPositive > countNegative:\n",
    "        return L_Positive\n",
    "\n",
    "    return L_Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aebe49d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.159994Z",
     "iopub.status.busy": "2025-02-16T11:27:05.159801Z",
     "iopub.status.idle": "2025-02-16T11:27:05.163699Z",
     "shell.execute_reply": "2025-02-16T11:27:05.163026Z"
    },
    "papermill": {
     "duration": 0.013651,
     "end_time": "2025-02-16T11:27:05.165084",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.151433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BATCH_SentimentCLM(model,tokenizer, dataset):\n",
    "    size = len(dataset)\n",
    "    predList = []\n",
    "    errCount=0\n",
    "    for i in dataset:\n",
    "        prompt = CLM_Prompt(i)\n",
    "        output = CLM_Sentiment(prompt, model, tokenizer,max_length=1200)\n",
    "#        print(\"Base=\", i['label'])\n",
    "#        print(output)\n",
    "#        index = output.find(SENTIMENT_SEP)\n",
    "        predLabel = PostProcess(output)\n",
    "        \"\"\"\n",
    "        if index!=-1:\n",
    "            iPositive = output[index:].lower().find(loPositive)\n",
    "            iNegative = output[index:].lower().find(loNegative)\n",
    "            if iPositive>0:\n",
    "                predLabel = L_Positive\n",
    "            elif iNegative>0:\n",
    "                predLabel = L_Negative\n",
    "            \n",
    "        if predLabel is None:\n",
    "            errCount = errCount+1\n",
    "            print('NOT_FOUND:', errCount) \n",
    "            predLabel = L_Positive\n",
    "        \"\"\"    \n",
    "        print(\"Out=\",predLabel)\n",
    "        predList.append(predLabel)\n",
    "    return predList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8420f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:27:05.182175Z",
     "iopub.status.busy": "2025-02-16T11:27:05.181953Z",
     "iopub.status.idle": "2025-02-16T11:30:20.735399Z",
     "shell.execute_reply": "2025-02-16T11:30:20.734257Z"
    },
    "papermill": {
     "duration": 195.563629,
     "end_time": "2025-02-16T11:30:20.736913",
     "exception": false,
     "start_time": "2025-02-16T11:27:05.173284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out= Positive\n",
      "Out= Negative\n"
     ]
    }
   ],
   "source": [
    "predList = None\n",
    "predList = BATCH_SentimentCLM(modelCLM,tokenizer, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65bc35f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:30:20.773211Z",
     "iopub.status.busy": "2025-02-16T11:30:20.772919Z",
     "iopub.status.idle": "2025-02-16T11:30:20.777449Z",
     "shell.execute_reply": "2025-02-16T11:30:20.776474Z"
    },
    "papermill": {
     "duration": 0.024184,
     "end_time": "2025-02-16T11:30:20.778820",
     "exception": false,
     "start_time": "2025-02-16T11:30:20.754636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def MakeSubmission(predList):\n",
    "\n",
    "    if predList is None:\n",
    "        predList = ['Positive'] * 100  # Set all labels to \"Positive\"\n",
    "    # Create a DataFrame with IDs from 1 to 100 and label \"Positive\"\n",
    "    data = {\n",
    "        'ID': range(1, len(predList)+1),  # Generate IDs from 1 to 100\n",
    "#        'label': ['Positive'] * 100  # Set all labels to \"Positive\"\n",
    "        'label': predList\n",
    "    }\n",
    "\n",
    "    # Create the DataFrame\n",
    "    submission_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save it to a CSV file\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0780bcb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:30:20.813671Z",
     "iopub.status.busy": "2025-02-16T11:30:20.813418Z",
     "iopub.status.idle": "2025-02-16T11:30:20.834453Z",
     "shell.execute_reply": "2025-02-16T11:30:20.833226Z"
    },
    "papermill": {
     "duration": 0.03988,
     "end_time": "2025-02-16T11:30:20.835759",
     "exception": false,
     "start_time": "2025-02-16T11:30:20.795879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID     label\n",
      "0     1  Positive\n",
      "1     2  Negative\n",
      "2     3  Positive\n",
      "3     4  Positive\n",
      "4     5  Negative\n",
      "..  ...       ...\n",
      "95   96  Negative\n",
      "96   97  Positive\n",
      "97   98  Negative\n",
      "98   99  Positive\n",
      "99  100  Negative\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "submission_df=MakeSubmission(predList)\n",
    "print(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a71ca9aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T11:30:20.871262Z",
     "iopub.status.busy": "2025-02-16T11:30:20.871023Z",
     "iopub.status.idle": "2025-02-16T11:30:20.873990Z",
     "shell.execute_reply": "2025-02-16T11:30:20.873362Z"
    },
    "papermill": {
     "duration": 0.021353,
     "end_time": "2025-02-16T11:30:20.875138",
     "exception": false,
     "start_time": "2025-02-16T11:30:20.853785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_Out = pd.DataFrame(predList,columns=['label'])\n",
    "#df_Out.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d162d0",
   "metadata": {
    "papermill": {
     "duration": 0.015806,
     "end_time": "2025-02-16T11:30:20.907756",
     "exception": false,
     "start_time": "2025-02-16T11:30:20.891950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a4f26",
   "metadata": {
    "papermill": {
     "duration": 0.016237,
     "end_time": "2025-02-16T11:30:20.940387",
     "exception": false,
     "start_time": "2025-02-16T11:30:20.924150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04cf24",
   "metadata": {
    "papermill": {
     "duration": 0.01682,
     "end_time": "2025-02-16T11:30:20.974505",
     "exception": false,
     "start_time": "2025-02-16T11:30:20.957685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11098970,
     "sourceId": 93282,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 333.959249,
   "end_time": "2025-02-16T11:30:24.079463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-16T11:24:50.120214",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0fcdde6333a549bba4c96405c443c8a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "114a46d2737d4b768d066a3cf85a0333": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1646172ae7d741de81ab303fe77161b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bc17ca3096d4074b06a17f967b01bf8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2fb67396230641d0b32cb9b9de8ffff8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "32e8d2e1a3834ee798e1e0d57c5ce1d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b14600a6f984db58d8d6593c462da55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5b533a80dbc3448b8365a807c3687f56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d98f4410bc9a4413ae94105fd0411547",
        "IPY_MODEL_f3f7709278f34c4ebedc49b20a6c0847",
        "IPY_MODEL_96f82063fbf64f9cb2d19350d0b9b157"
       ],
       "layout": "IPY_MODEL_1646172ae7d741de81ab303fe77161b0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "96f82063fbf64f9cb2d19350d0b9b157": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_114a46d2737d4b768d066a3cf85a0333",
       "placeholder": "​",
       "style": "IPY_MODEL_0fcdde6333a549bba4c96405c443c8a7",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [01:16&lt;00:00, 16.48s/it]"
      }
     },
     "d98f4410bc9a4413ae94105fd0411547": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1bc17ca3096d4074b06a17f967b01bf8",
       "placeholder": "​",
       "style": "IPY_MODEL_2fb67396230641d0b32cb9b9de8ffff8",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "f3f7709278f34c4ebedc49b20a6c0847": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32e8d2e1a3834ee798e1e0d57c5ce1d0",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5b14600a6f984db58d8d6593c462da55",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
