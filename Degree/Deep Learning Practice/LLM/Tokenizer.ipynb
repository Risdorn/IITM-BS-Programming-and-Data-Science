{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caf9ab1-70c4-4331-adab-36632e624d18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "**Execute** the three code cells below before switching to presentation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa8a406",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Layout, interact, interactive, fixed, interact_manual, widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800f6ca9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ff519c-95fc-4e85-92ad-9b8d5c4b2e9d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to display a pair of subtokens to be merged in a slider\n",
    "def get_pairs(pair:int):\n",
    "    \"\"\"\n",
    "    pair: index of the pair. \n",
    "    \"\"\"\n",
    "    if pair>0:\n",
    "        left, right = lines[pair].strip('\\n').split(' ')\n",
    "        print(f'{left} , {right}')\n",
    "        \n",
    "# to display token ids  in a slider\n",
    "def display_token_id(id):\n",
    "    token,id = vocab_sorted[id]\n",
    "    print(f'id:{id} \\t token:{token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a1173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dda012",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76563d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We will use the `bookcorpus` dataset to train a tokenizer. It may take about 15 minutes or more to download and generate the split for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d961176",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It requires approximately 5 GB of memory to load the dataset. Please ensure you have sufficient memory available. If not, consider loading only a fraction of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedac42e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 74004228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('bookcorpus',split='all')\n",
    "pprint(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff1eac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The dataset contains 74 million sentences of varying lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9da4b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The length of sentences does not affect the training of a tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca57d52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's take a look at a few examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c880eeca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : usually , he would be tearing around the living room , playing with his toys .\n",
      "1 : but just one look at a minion sent him practically catatonic .\n",
      "2 : that had been megan 's plan when she got him dressed earlier .\n",
      "3 : he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\n",
      "4 : she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .\n",
      "5 : `` are n't you being a good boy ? ''\n"
     ]
    }
   ],
   "source": [
    "num_samples = 6\n",
    "for idx,sample in enumerate(ds[0:num_samples]['text']):\n",
    "    print(f'{idx} : {sample}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30b769",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Tokenization </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b632a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Recall that a `tokenization` pipeline consists of Normalizer, Pre-Tokenizer, Model and Post-Processor as show below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54869d13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/pipeline.png align='left'> <br> <br> <br> <br> <br> Let us build this pipeline by importing the `Tokenizer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0160b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158fb75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's use a simple tokenizer with the following choices for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444a658",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "|**Component** |**Choice**  |\n",
    "|:------------:|:----------:|\n",
    "|normalizer    |Lowercase   |\n",
    "|pre-tokenizer |Whitespace  |\n",
    "|model         | BPE        |\n",
    "|postprocessor | None       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5436b42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import Lowercase \n",
    "from tokenizers.pre_tokenizers import Whitespace \n",
    "from tokenizers.models import BPE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5306049",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Initiate the tokenizer with the BPE model and the special tokens (\"[UNK]\" in this case) that the model will use during **prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84ce283",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BPE(unk_token=\"[UNK]\")\n",
    "tokenizer = Tokenizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e907e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Add the normalizer and pre-tokenizer to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b727811",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed53bf4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Create a trainer by setting `vocab_size` and `special_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1db46f43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(vocab_size=32000,special_tokens=[\"[PAD]\",\"[UNK]\"],continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca962a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The pipeline is ready. Next,we need to pass **list** of strings as input to the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e618d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "However, creating an additional **list** containing all 74 million samples will require an extra 5 GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae079a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "To reduce the memory usage, let us create a generator that returns a batch of samples as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3befee39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_examples(batch_size=1000):\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i : i + batch_size]['text']    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc19122",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let us train the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d43c68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/trainer.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25776845",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "print(cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "963f68d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_examples(batch_size=10000),trainer=trainer,length=len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56405e24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The training took about 5 minutes to complete and required an additional 1 GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93fded",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It would be interesting to see the subtokens that were merged to create the final vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfa8d80f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/hopper-vocab.json', 'model/hopper-merges.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('model',prefix='hopper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba05b2c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#version: 0.2\n",
      "\n",
      "##h ##e\n",
      "\n",
      "t ##he\n",
      "\n",
      "##i ##n\n",
      "\n",
      "##e ##r\n",
      "\n",
      "##e ##d\n",
      "\n",
      "##o ##u\n",
      "\n",
      "##n ##d\n",
      "\n",
      "##in ##g\n",
      "\n",
      "t ##o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('model/hopper-merges.txt','r') as file:\n",
    "    row = 0\n",
    "    num_lines = 10\n",
    "    for line in file.readlines():\n",
    "        print(line)\n",
    "        row+=1 \n",
    "        if row >= num_lines:\n",
    "            break   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97388636",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Display last `n` merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c76cb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel ##anthe\n",
      "\n",
      "black ##er\n",
      "\n",
      "ad ##ject\n",
      "\n",
      "v ##ang\n",
      "\n",
      "betroth ##al\n",
      "\n",
      "tiptoe ##ing\n",
      "\n",
      "restroom ##s\n",
      "\n",
      "consol ##ing\n",
      "\n",
      "esp ##ionage\n",
      "\n",
      "influ ##x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('model/hopper-merges.txt','r') as file:\n",
    "    row = 0\n",
    "    num_lines = 10\n",
    "    for line in reversed(file.readlines()):\n",
    "        print(line)\n",
    "        row+=1\n",
    "        if row >= num_lines:\n",
    "            break   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52b766",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Vocabulary </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f0bc3-bbf5-4ffb-b456-ca84597324d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's view the number of merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db1ec64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('model/hopper-merges.txt','r') as file:    \n",
    "    lines = file.readlines()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cbcf171",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merges:31871\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of merges:{len(lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa357dc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:32000\n"
     ]
    }
   ],
   "source": [
    "print(f'vocab size:{tokenizer.get_vocab_size()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb33688",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The number of merges is slightly less than the size of the vocabulary because the merges do not include single-character tokens, such as letters, numbers, and special symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2050a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can view the final learned vocabulary either from the saved `hopper-vocab.json` file or by using the `get_vocab` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ea8b844",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57082bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For convenience, let's print the vocabulary sorted by token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be3b5efe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb5e33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's adjust the sliders below to view the merged subwords and their corresponding tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af12e315",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23b1774a8ec4796b0d603b502d283ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='pair', layout=Layout(width='900px'), max=31870, min=1), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed9e16a05464cf7bdf668cbe8863562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=130, description='id', layout=Layout(width='900px'), max=31999), Output(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = interact(get_pairs,pair=widgets.IntSlider(min=1, max=len(lines)-1, step=1, value=1,layout=Layout(width='900px')))\n",
    "_ = interact(display_token_id,id=widgets.IntSlider(min=0, max=31999, step=1, value=130,layout=Layout(width='900px')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f799b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Note that the 130th ID represents the first merge. What will the next two tokens to be merged be? You can explore this by moving the sliders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b8e66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The sub-words are `t` and `##he`. After the merge, they will form the token `the`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e6830",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Similarly the last two merges are: `mel ##anthe` and `black ##er` (Move the slider to the far right to see them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5282df2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Encoding </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb252fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Pass a single sample to the `encode` method of the tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0120fb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: usually , he would be tearing around the living room , playing with his toys .\n",
      "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "sample = ds[0]['text']\n",
    "print(f'sample: {sample}')\n",
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8f921",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It returned the output as `Encoding` object that contains useful attributes such as `token_ids` (ids), `type_ids` and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9645af6-0a35-4fe6-8a13-ac689c022afb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We need to access these attributes to get their respective values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58235616",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_ids = encoding.ids\n",
    "tokens = encoding.tokens\n",
    "type_ids = encoding.type_ids\n",
    "attention_mask = encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1485fae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >usually</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >he</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >would</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >be</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tearing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >around</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >living</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >room</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >playing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >his</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >toys</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "visualizer = EncodingVisualizer(tokenizer=tokenizer)\n",
    "visualizer(text=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2da3817b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ids</th>\n",
       "      <th>type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually</td>\n",
       "      <td>2462</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tearing</td>\n",
       "      <td>6456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>around</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>living</td>\n",
       "      <td>1559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>room</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>playing</td>\n",
       "      <td>2301</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>with</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>his</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>toys</td>\n",
       "      <td>9774</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens   ids  type_ids  attention_mask\n",
       "0   usually  2462         0               1\n",
       "1         ,    19         0               1\n",
       "2        he   149         0               1\n",
       "3     would   277         0               1\n",
       "4        be   162         0               1\n",
       "5   tearing  6456         0               1\n",
       "6    around   422         0               1\n",
       "7       the   131         0               1\n",
       "8    living  1559         0               1\n",
       "9      room   536         0               1\n",
       "10        ,    19         0               1\n",
       "11  playing  2301         0               1\n",
       "12     with   201         0               1\n",
       "13      his   177         0               1\n",
       "14     toys  9774         0               1\n",
       "15        .    21         0               1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict = {'tokens':tokens,'ids':token_ids,'type_ids':type_ids,'attention_mask':attention_mask}\n",
    "df = pd.DataFrame.from_dict(out_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438acc02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`type_ids` are model-specific. For instance, BERT-like models use type IDs of 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c707a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Another important attribute is the `attention_mask`, which is used in nearly all transformer-based architectures. In this mask, the value is 1 for tokens to be attended to and 0 for masked tokens (which may seem counterintuitive given the term \"masking\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55795dcc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\">  Batch Encoding </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d020a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "When encoding a batch of samples, we need to`[PAD]` shorter sequences in the batch, a process known as dynamic batching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40e53e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Therefore, we use `encode_batch` method of the tokenizer object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86559b52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The `token_id` for the `[PAD]` token is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c6b1e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = ds[0:4]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "133aed35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "pprint(batch_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db10be29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The maximum length of the sequence in the batch is 42. Clearly, padding is not applied to the remaining samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8d181",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In general, it is also possible for the length of a sequence to exceed the model's context length (or window size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a8cdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Assuming the model's context length is 512, enable padding and truncation while batching the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91cf63e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all default args\n",
    "tokenizer.enable_padding(direction = 'right',\n",
    "                         pad_id = 0,\n",
    "                         pad_type_id = 0,\n",
    "                         pad_token = '[PAD]',\n",
    "                         length = None, # None default to max_len in the batch\n",
    "                         pad_to_multiple_of = None) \n",
    "\n",
    "tokenizer.enable_truncation(max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fce485",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "pprint(batch_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4469f-23a5-4bf1-8401-92811800fbb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Now we can see that all samples in the batch are of the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10efb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\">  Quick test </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d9cc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's pass a test sequence that contains two tokens not present in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34418e84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'this', 'is', 'so', 'simple', 'to', 'do', 'in', 'h', '##f', '[UNK]', '[UNK]', '##.']\n"
     ]
    }
   ],
   "source": [
    "text = \"All this is so simple to do in HF இ😊.\"\n",
    "encoded = tokenizer.encode(text).tokens\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfedc6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Normalization worked as expected (This $\\rightarrow$ this, HF $\\rightarrow$ hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df28974",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Pre-tokenization followed by model tokenization has been applied correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e295e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The continuing prefix has been used appropriately. ('##f', '##.`) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9d912",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\">  Saving and Loading Tokenizer </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02d022",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let us save the tokenizer and load it in a single line of code in the model training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c85e8075",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save('hopper.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28fddb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It saves all the required information such as `added_tokens`, details of the `model` (vocab,merges,..) , `normalizer`, `pre-tokenizer` .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1097099",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('hopper.json','r') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ddbabf2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'added_tokens': [...],\n",
      " 'decoder': None,\n",
      " 'model': {...},\n",
      " 'normalizer': {...},\n",
      " 'padding': {...},\n",
      " 'post_processor': None,\n",
      " 'pre_tokenizer': {...},\n",
      " 'truncation': {...},\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "pprint(json_data, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79656710",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It is now easy to load the tokenizer back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e1b419f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91799fe0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_tokenizer = trained_tokenizer.from_file('hopper.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aaf737e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'this', 'is', 'so', 'simple', 'to', 'do', 'in', 'h', '##f', '[UNK]', '[UNK]', '##.']\n"
     ]
    }
   ],
   "source": [
    "tokens = trained_tokenizer.encode(text).tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e1186",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> BERT Tokenizer </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e7c20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let’s quickly build a BERT-like tokenizer (don’t worry about what BERT-like models are for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfc75b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The input to BERT models generally follows the template below, with slight variations across different BERT implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423537d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "[ `[CLS]`, token-A_1, $\\cdots$, token-A_n, `[SEP]`, token-B_1, $\\cdots$, token-B_m ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bc12b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Here `A` denotes sentence `A` with `n` tokens and `B` denotes  sentence `B` with `m` tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72cc6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The special tokens are : `[CLS]`,`[SEP]`,`[PAD]`,`[MASK]` and `[UNK]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b710dce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "bert_tokenizer.normalizer = Lowercase()\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "bert_trainer = BpeTrainer(vocab_size=32000,\n",
    "                          special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"],\n",
    "                          continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d40e26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We just need to add `post_processing_step` where the special tokens are inserted according to  the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "133f550d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd420b9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* If we pass a single sentence, the tokenizer should output `\"[CLS] $0 [SEP]\"` where 0 denotes the `type_id` (which defaults to zero if there is a single sentence) <br>\n",
    "\n",
    "* If we pass a pair of sentence, the tokenizer should output `\"[CLS] $A:0 [SEP] $B:1\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a702026",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $0 [SEP]\",\n",
    "                                                   pair=\"[CLS] $A [SEP] $B:1\",\n",
    "                                                   special_tokens=[(\"[CLS]\", 2), (\"[SEP]\", 3)],\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c53115c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer.train_from_iterator(get_examples(batch_size=10000),trainer=bert_trainer,length=len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1d148",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Pass a single sentence to the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "572f4d2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [2, 270, 956, 336, 231, 2534, 141, 206, 157, 56, 95, 24, 462, 17, 67,\n",
      "         206, 387, 3],\n",
      " 'tokens': ['[CLS]', 'all', 'these', 'are', 'so', 'simple', 'to', 'do', 'in',\n",
      "            'h', '##f', '.', 'let', \"'\", 's', 'do', 'more', '[SEP]']}\n"
     ]
    }
   ],
   "source": [
    "text = \"All these are so simple to do in HF. Let's do more\"\n",
    "encoded = bert_tokenizer.encode(text)\n",
    "tokens = encoded.tokens\n",
    "ids = encoded.ids\n",
    "out_dict = {'tokens':tokens,'ids':ids}\n",
    "pprint(out_dict,depth=2,compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965375e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "pass a pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44d655c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [2, 270, 956, 336, 231, 2534, 141, 206, 157, 56, 95, 24, 462, 17, 67,\n",
      "         206, 387, 3, 214, 250, 49, 490, 415, 141, 260, 12],\n",
      " 'tokens': ['[CLS]', 'all', 'these', 'are', 'so', 'simple', 'to', 'do', 'in',\n",
      "            'h', '##f', '.', 'let', \"'\", 's', 'do', 'more', '[SEP]', 'we',\n",
      "            'have', 'a', 'long', 'way', 'to', 'go', '!']}\n"
     ]
    }
   ],
   "source": [
    "text = \"All these are so simple to do in HF. Let's do more\"\n",
    "pair = \"We have a long way to go!\"\n",
    "encoded = bert_tokenizer.encode(text,pair)\n",
    "tokens = encoded.tokens\n",
    "ids = encoded.ids\n",
    "out_dict = {'tokens':tokens,'ids':ids}\n",
    "pprint(out_dict,depth=2,compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f59631",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Decoding </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054d90c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The special tokens (for ex, `[PAD]`) need to be removed and sub-words have to be merged before outputting the final result to the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da137d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let us decode the enoced `token_ids` from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf4441e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all these are so simple to do in h ##f . let ' s do more we have a long way to go !\n"
     ]
    }
   ],
   "source": [
    "plain_tokens = bert_tokenizer.decode(ids)\n",
    "print(plain_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ba11c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The special tokens were removed, but the subword `(h, ##f)` wasn't merged into a complete word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa3429",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We need to use appropriate decoders based on the type of tokenizer. See the list of decoders [here](https://huggingface.co/docs/tokenizers/v0.13.4.rc2/en/api/decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9d0aefe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.decoders import WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00917bbc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.decoder = WordPiece(prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf4c9dbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all these are so simple to do in hf. let ' s do more we have a long way to go!\n"
     ]
    }
   ],
   "source": [
    "plain_tokens = bert_tokenizer.decode(ids)\n",
    "print(plain_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87863a1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\">  Pretrained Tokenizer </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5f12f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Finally, we need to wrap everything in a `PreTrainedTokenizer` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d90e63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Recall that using `tokenizer.encode` returns `Encoding` with a list of attributes that includes `ids`, `ids` `tokens`, `attention_mask` and many more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53eeeff3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(text)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa81e90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In general, the model requires the `input_ids`,`attention_mask` and other model specific attributes. To get these from the `Encoding` object we need to iterate over all the `Encoding` objects corresponding to each sample in a batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113de60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here comes the `PreTrainedTokenizer` class to rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bbb2ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "150ee79b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlp/.dlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pt_tokenizer = PreTrainedTokenizerFast(tokenizer_file='hopper.json',\n",
    "                                      unk_token='[UNK]',\n",
    "                                      pad_token='[PAD]',\n",
    "                                      model_input_names=[\"input_ids\",\"token_type_ids\",\"attention_mask\"],\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826b623",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Now we can simply call the `pt_tokenizer` with an input. See the call signature [here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d9da296",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [267, 953, 333, 228, 2531, 138, 203, 154, 53, 96, 21, 459, 14, 64,\n",
      "               203, 384],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = pt_tokenizer(text)\n",
    "pprint(model_inputs,compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a17fec79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1],\n",
      " 'input_ids': [267, 953, 333, 228, 2531, 138, 203, 154, 53, 96, 21, 459, 14, 64,\n",
      "               203, 384, 211, 247, 46, 487, 412, 138, 257, 9],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = pt_tokenizer(text,text_pair=pair)\n",
    "pprint(model_inputs,compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296fb87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Note that there is an additional `type id`. There are no special tokens `[CLS]:2`,`[SEP]:3` as we haven't used `bert_tokenizer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41213c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can also pass a **batch** of samples and it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "100a686a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_text = ['I like the book The Psychology of Money','I enjoyed watching the Transformers movie','oh! thanks for this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c734b0ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[54, 281, 131, 1701, 131, 19478, 153, 1564],\n",
      "               [54, 4096, 1443, 131, 7744, 307, 3760],\n",
      "               [772, 9, 1767, 200, 254]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = pt_tokenizer(batch_text)\n",
    "pprint(model_inputs,compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561642b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Padding is not done by default so let's enable padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b0ebf93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "                    [1, 1, 1, 1, 1, 0, 0, 0]],\n",
      " 'input_ids': [[54, 281, 131, 1701, 131, 19478, 153, 1564],\n",
      "               [54, 4096, 1443, 131, 7744, 307, 3760, 0],\n",
      "               [772, 9, 1767, 200, 254, 0, 0, 0]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = pt_tokenizer(batch_text,padding=True)\n",
    "pprint(model_inputs,compact=True)"
   ]
  },
{
   "cell_type": "markdown",
   "id": "bebd957d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can save the pre-trained tokenizer by calling `pt_tokenizer.save('hopper')`.It will create a directory named `hopper` and store all the required files ( `tokenizer.json`, `tokenizer_config.json`, `special_tokens_map.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd958d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`PreTrainedTokenizer` class implements additional methods that are useful for the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241ac9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We just need to pass the `tokenizer_file` and other few arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4945d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We will see how to make use of `PreTrainedTokenizer` for training a Model in the next experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
