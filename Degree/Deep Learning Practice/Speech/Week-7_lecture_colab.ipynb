{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b85cbb1e4ebb4b9388491fe53a029216":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c16064cb2f3f46b587e0abf2db11d26b","IPY_MODEL_ebb795498bef483a84bb256de3d64ddc","IPY_MODEL_ad2c8bde1ec64b04a5878ef178eaf9da"],"layout":"IPY_MODEL_6978f8bb9b38416bb6d3a1f1d6f0b219"}},"c16064cb2f3f46b587e0abf2db11d26b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4afd206e562f400fad0cdb0616b9893f","placeholder":"​","style":"IPY_MODEL_1105ce64cc4346bab56bc4f0cc28936c","value":"Map: 100%"}},"ebb795498bef483a84bb256de3d64ddc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b88d2929cab4897a5af5a9f1edc8937","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b969cc88812441138339a526ad6fec5d","value":8000}},"ad2c8bde1ec64b04a5878ef178eaf9da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de465e51d9144e41beb20c4388e6304f","placeholder":"​","style":"IPY_MODEL_74587a17da784200889a722e8ef32a5b","value":" 8000/8000 [00:03&lt;00:00, 2656.58 examples/s]"}},"6978f8bb9b38416bb6d3a1f1d6f0b219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4afd206e562f400fad0cdb0616b9893f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1105ce64cc4346bab56bc4f0cc28936c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b88d2929cab4897a5af5a9f1edc8937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b969cc88812441138339a526ad6fec5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de465e51d9144e41beb20c4388e6304f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74587a17da784200889a722e8ef32a5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa454e3e0e584a8785a6838fd35035e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4379dff1374f4bc6950162e7f8fe9dc8","IPY_MODEL_226cc5d12aa24db2a638693ed75e1edc","IPY_MODEL_41af2fa7a2414e00b5469336afe067c7"],"layout":"IPY_MODEL_437d97ae3fed4b93be22562541beceb7"}},"4379dff1374f4bc6950162e7f8fe9dc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bec77c08883949afae60a7bf4d29cbad","placeholder":"​","style":"IPY_MODEL_7f00377525ec4e64bb52affa8a52178d","value":"Map: 100%"}},"226cc5d12aa24db2a638693ed75e1edc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7acb11be0c6144fc9dc13c03667547b3","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e639439f50541dd8abca7b4ba74d68f","value":8000}},"41af2fa7a2414e00b5469336afe067c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2986d6f222844a4380397a0ea7d02c67","placeholder":"​","style":"IPY_MODEL_e62ea2a3dc0f4469988815da52c2333a","value":" 8000/8000 [00:50&lt;00:00, 487.39 examples/s]"}},"437d97ae3fed4b93be22562541beceb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bec77c08883949afae60a7bf4d29cbad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f00377525ec4e64bb52affa8a52178d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7acb11be0c6144fc9dc13c03667547b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e639439f50541dd8abca7b4ba74d68f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2986d6f222844a4380397a0ea7d02c67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e62ea2a3dc0f4469988815da52c2333a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Installing the required packages"],"metadata":{"id":"APzvV0qt9wWI"}},{"cell_type":"code","source":["%pip install datasets transformers evaluate jiwer"],"metadata":{"id":"mO4JqLUZPRje"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygJeEmwLO_P8"},"outputs":[],"source":["import torch\n","import datasets\n","import evaluate\n","import numpy as np\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","from transformers import '+', TrainingArguments, Trainer"]},{"cell_type":"markdown","source":["Explore the Data online [here](https://huggingface.co/datasets/SPRINGLab/asr-task-data)."],"metadata":{"id":"e7uIGK8G_Zfm"}},{"cell_type":"code","source":["dataset = datasets.load_dataset(\"SPRINGLab/asr-task-data\")"],"metadata":{"id":"anIIhuGBPL_w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function takes a batch of data as input. Each batch contains text samples. It concatenates all the text samples in the batch into a single string `all_text`. It then creates a list of unique characters `vocab` from this combined text and returns a dictionary with two keys: `vocab`, containing the list of unique characters, and `all_text`, containing the combined text."],"metadata":{"id":"qTwptQvfBNHV"}},{"cell_type":"code","source":["def extract_all_chars(batch):\n","  all_text = \" \".join(batch[\"text\"])\n","  vocab = list(set(all_text))\n","  return {\"vocab\": [vocab], \"all_text\": [all_text]}"],"metadata":{"id":"9EVAM0MoWVrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabs = dataset.map(extract_all_chars, batch_size=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b85cbb1e4ebb4b9388491fe53a029216","c16064cb2f3f46b587e0abf2db11d26b","ebb795498bef483a84bb256de3d64ddc","ad2c8bde1ec64b04a5878ef178eaf9da","6978f8bb9b38416bb6d3a1f1d6f0b219","4afd206e562f400fad0cdb0616b9893f","1105ce64cc4346bab56bc4f0cc28936c","5b88d2929cab4897a5af5a9f1edc8937","b969cc88812441138339a526ad6fec5d","de465e51d9144e41beb20c4388e6304f","74587a17da784200889a722e8ef32a5b"]},"id":"IdNdkqPd16RE","outputId":"de286bbe-fc97-4ad0-da9b-2345c33be50f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85cbb1e4ebb4b9388491fe53a029216"}},"metadata":{}}]},{"cell_type":"markdown","source":["`vocab_list` is initialized to hold all characters found across the dataset.\n","We iterate over the vocabularies from each batch in the training set and extends `vocab_list` with the characters from each batch's vocabulary."],"metadata":{"id":"irQ_mQ5dB7wA"}},{"cell_type":"code","source":["vocab_list = []"],"metadata":{"id":"mTtRaJUd6F06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for v in vocabs[\"train\"][\"vocab\"]:\n","  vocab_list.extend(v[0])"],"metadata":{"id":"S-GgvE6f6BTl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This converts vocab_list into a set to remove any duplicate characters and then back into a list."],"metadata":{"id":"9EOKsxJrCLDx"}},{"cell_type":"code","source":["vocab_list = list(set(vocab_list))"],"metadata":{"id":"QJC0rBwF4ufZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This creates a dictionary vocab_dict where each unique character is mapped to a unique integer ID as tokens, starting from 0."],"metadata":{"id":"LlYu3F4ACPrg"}},{"cell_type":"code","source":["vocab_dict = {v: k for k, v in enumerate(vocab_list)}"],"metadata":{"id":"1OABWqR55f1l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This merges the mapping for the space character \" \" with the pipe character \"|\". The space character is deleted from the dictionary after its ID is reassigned to the pipe character.   \n","\n","To make it clearer that \" \" has its own token class, we give it a more visible character \"|\". In addition, we also add an \"[UNK]\" token so that the model can later deal with characters not encountered in the training set."],"metadata":{"id":"tBqNyOjaCYE4"}},{"cell_type":"code","source":["vocab_dict[\"|\"] = vocab_dict[\" \"]\n","del vocab_dict[\" \"]"],"metadata":{"id":"lTmCB9bg6cNQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_dict[\"[UNK]\"] = len(vocab_dict)\n","vocab_dict[\"[PAD]\"] = len(vocab_dict)\n","len(vocab_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3uyA7UJo6om5","outputId":"17e9dceb-ec47-466d-96f4-baac14c765a1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["import json\n","with open('vocab.json', 'w') as vocab_file:\n","    json.dump(vocab_dict, vocab_file)"],"metadata":{"id":"-CbnJkzc6sE8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A `Wav2Vec2CTCTokenizer` is created using the saved vocab.json file.  \n","\n","It defines the following special tokens:  \n","unk_token: \"[UNK]\" for unknown characters.  \n","pad_token: \"[PAD]\" for padding.  \n","word_delimiter_token: \"|\", which is used as the token for separating words (since spaces were replaced by pipes earlier).   \n","\n","[🤗 Doc](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer) for Wav2Vec2 Tokenizer."],"metadata":{"id":"f8PcLLd_HIyh"}},{"cell_type":"code","source":["tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"],"metadata":{"id":"QAeI4i9v6xTP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `Wav2Vec2FeatureExtractor` is initialized to process audio features.\n","\n","Key parameters:  \n","feature_size=1: Indicates 1D audio features.  \n","sampling_rate=16000: Audio is expected to have a sampling rate of 16,000 Hz.   \n","padding_value=0.0: Padding values are set to 0.0.  \n","do_normalize=True: Normalization of audio is enabled.  \n","return_attention_mask=False: Attention masks are not used.  \n","\n","[🤗 Docs](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) for Wav2Vec2FeatureExtractor"],"metadata":{"id":"a_OfgxSGHu1Q"}},{"cell_type":"code","source":["feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"],"metadata":{"id":"wHMrQB9R7tzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`Wav2Vec2Processor` combines feature extractor and tokenizer.  \n","\n","[🤗 Docs](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) for Wav2Vec2Processor"],"metadata":{"id":"55bu9v77IgEf"}},{"cell_type":"code","source":["processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"],"metadata":{"id":"VXUnQ8So7ywq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Target text:\", dataset[\"train\"][56][\"text\"])\n","print(\"Input array shape:\", np.asarray(dataset[\"train\"][56][\"audio\"][\"array\"]).shape)\n","print(\"Sampling rate:\", dataset[\"train\"][56][\"audio\"][\"sampling_rate\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVgQZdKV8EiQ","outputId":"9369e899-4191-467a-e16b-6fdafe89dbcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Target text: the first location within my training data set now in order to do it for training i would\n","Input array shape: (76624,)\n","Sampling rate: 16000\n"]}]},{"cell_type":"markdown","source":["This function prepares the data for training by extracting features."],"metadata":{"id":"x_4Oyb5plSiN"}},{"cell_type":"code","source":["def prepare_dataset(batch):\n","    audio = batch[\"audio\"]\n","    # batched output is \"un-batched\" to ensure mapping is correct\n","    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n","    batch[\"input_length\"] = len(batch[\"input_values\"])\n","\n","    with processor.as_target_processor():\n","        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n","    return batch"],"metadata":{"id":"03oaDWUE8TWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.map(prepare_dataset, batch_size=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["fa454e3e0e584a8785a6838fd35035e7","4379dff1374f4bc6950162e7f8fe9dc8","226cc5d12aa24db2a638693ed75e1edc","41af2fa7a2414e00b5469336afe067c7","437d97ae3fed4b93be22562541beceb7","bec77c08883949afae60a7bf4d29cbad","7f00377525ec4e64bb52affa8a52178d","7acb11be0c6144fc9dc13c03667547b3","3e639439f50541dd8abca7b4ba74d68f","2986d6f222844a4380397a0ea7d02c67","e62ea2a3dc0f4469988815da52c2333a"]},"id":"-gP4w0sv8cRn","outputId":"aa80b744-97bc-4a23-81c1-a1705df4e91b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa454e3e0e584a8785a6838fd35035e7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["@dataclass\n","class DataCollatorCTCWithPadding:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received.\n","    Args:\n","        processor (:class:`~transformers.Wav2Vec2Processor`)\n","            The processor used for proccessing the data.\n","        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence if provided).\n","            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","              maximum acceptable input length for the model if that argument is not provided.\n","            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","              different lengths).\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lenghts and need\n","        # different padding methods\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            return_tensors=\"pt\",\n","        )\n","        with self.processor.as_target_processor():\n","            labels_batch = self.processor.pad(\n","                label_features,\n","                padding=self.padding,\n","                return_tensors=\"pt\",\n","            )\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"],"metadata":{"id":"vuGytmu683iJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"],"metadata":{"id":"ubE33egL9V-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define a error metric and function to compute it while training."],"metadata":{"id":"gtrYUq0jlf5O"}},{"cell_type":"code","source":["wer_metric = evaluate.load(\"wer\")"],"metadata":{"id":"YgYiBZJk9Xz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(pred):\n","    pred_logits = pred.predictions\n","    pred_ids = np.argmax(pred_logits, axis=-1)\n","\n","    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n","\n","    pred_str = processor.batch_decode(pred_ids)\n","    # we do not want to group tokens when computing the metrics\n","    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n","\n","    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"],"metadata":{"id":"FlhaHUIO948M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = dataset['train'].train_test_split(test_size=0.1, shuffle=True, seed=42)"],"metadata":{"id":"ttUh0Wag-0lc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Base Pretrained model is defined and parameters are set for training.  \n","\n","Refer to [🤗 Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer) and [🤗 TrainingArguments](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.TrainingArguments) to learn about the Parameters."],"metadata":{"id":"9PgrWpZ8ll15"}},{"cell_type":"code","source":["model = AutoModelForCTC.from_pretrained(\n","    \"facebook/wav2vec2-base\",\n","    ctc_loss_reduction=\"mean\",\n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=len(processor.tokenizer)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xrLfzHtD99Y_","outputId":"09c32e5a-6052-45c7-eded-fb0079000489"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["model.freeze_feature_encoder()"],"metadata":{"id":"85CDjoz0-G_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","  output_dir=\"AsrTaskModel\",\n","  group_by_length=True,\n","  per_device_train_batch_size=8,\n","  per_device_eval_batch_size=8,\n","  evaluation_strategy=\"steps\",\n","  num_train_epochs=7,\n","  fp16=True,\n","  gradient_checkpointing=True,\n","  save_steps=1000,\n","  eval_steps=500,\n","  logging_steps=500,\n","  learning_rate=1e-4,\n","  weight_decay=0.005,\n","  warmup_steps=1000,\n","  save_total_limit=2,\n","  load_best_model_at_end=True,\n","  save_strategy=\"steps\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0GUcWKB-Luy","outputId":"10257497-72ec-43fc-a326-882c9e858c17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=processor.feature_extractor,\n",")"],"metadata":{"id":"Upmvu52G-uSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":997},"id":"lan6_69l7TD_","outputId":"ebc7969f-5a79-44a4-f373-94db82d65ec4"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1463' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1463/9000 21:02 < 1:48:31, 1.16 it/s, Epoch 1.62/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Wer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.575200</td>\n","      <td>3.073219</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.920800</td>\n","      <td>0.961972</td>\n","      <td>0.556434</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6001' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6001/9000 1:24:37 < 42:18, 1.18 it/s, Epoch 6.67/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Wer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.575200</td>\n","      <td>3.073219</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.920800</td>\n","      <td>0.961972</td>\n","      <td>0.556434</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.851400</td>\n","      <td>0.712503</td>\n","      <td>0.445027</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.667200</td>\n","      <td>0.614925</td>\n","      <td>0.351242</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.548200</td>\n","      <td>0.602136</td>\n","      <td>0.351586</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.468000</td>\n","      <td>0.499315</td>\n","      <td>0.291498</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.440400</td>\n","      <td>0.499604</td>\n","      <td>0.280667</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.384000</td>\n","      <td>0.514028</td>\n","      <td>0.273618</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.361700</td>\n","      <td>0.449441</td>\n","      <td>0.262013</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.305000</td>\n","      <td>0.470037</td>\n","      <td>0.257801</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.293900</td>\n","      <td>0.453271</td>\n","      <td>0.248173</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='53' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 53/100 00:29 < 00:26, 1.75 it/s]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PVVBVuyyAuEi"},"execution_count":null,"outputs":[]}]}