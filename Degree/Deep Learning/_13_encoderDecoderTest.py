from _06_layers import rnnLayer, rnnEncoder
import numpy as np

parameters = np.load('Degree/Deep Learning/parameters_w11.npz')

# This is the assignment question
# encode ariya and decode learn. The sequence is generated by the RNN layer
# ariya is represented as
xe = [np.array([1, 0, 0, 0, 0]), np.array([0, 1, 0, 0, 0]), np.array([0, 0, 1, 0, 0]), np.array([0, 0, 0, 1, 0]), np.array([1, 0, 0, 0, 0])]
# learn is represented as, <go>, l, e, a, r
initialState = np.array([1, 0, 0, 0, 1]) # Represents go
initialState = initialState.reshape(5, 1)
xd = [np.array([1, 0, 0, 0, 1]), np.array([1, 0, 0, 0, 0]), np.array([0, 1, 0, 0, 0]), np.array([0, 0, 1, 0, 0]), np.array([0, 0, 0, 1, 0])]
yd = [np.array([1, 0, 0, 0, 0]), np.array([0, 1, 0, 0, 0]), np.array([0, 0, 1, 0, 0]), np.array([0, 0, 0, 1, 0]), np.array([0, 0, 0, 0, 1])]
words = {0: 'l', 1: 'e', 2: 'a', 3: 'r', 4: 'n'}
for i in range(5):
    xd[i] = xd[i].reshape(5, 1)
    xe[i] = xe[i].reshape(5, 1)
    yd[i] = yd[i].reshape(5, 1)

input_size = 5
hidden_size = 5
output_size = 5
# encoder and decoder models
encoderModel = rnnEncoder(input_size, hidden_size, output_size, random_seed=0)
decoderModel = rnnLayer(input_size, hidden_size, output_size, random_seed=0)
encoderModel.input_weights = parameters.get('U_e')
encoderModel.input_bias = np.zeros((hidden_size, 1))
encoderModel.hidden_weights = parameters.get('W_e')
decoderModel.input_weights = parameters.get('U_d')
decoderModel.input_bias = np.zeros((hidden_size, 1))
decoderModel.hidden_weights = parameters.get('W_d')
decoderModel.output_weights = parameters.get('V_d')
decoderModel.output_bias = np.zeros((output_size, 1))

# First pass xe through encoder
encoder_output = encoderModel.forward_constant(xe, 5)
# Set initial hidden state of decoder to the last hidden state of encoder
decoderModel.hidden[0] = encoder_output[-1]
decoder_output = decoderModel.forward_constant(xd, 5)
loss = 0
for i in range(5):
    j = np.argmax(yd[i])
    print("Loss at time step", i+1, ":", -np.log(decoder_output[i][j]))
    loss += -np.log(decoder_output[i][j])
print('Initial Loss:', loss)

decoderModel.hidden = [encoder_output[-1]]
decoder_output = decoderModel.forward(initialState, 5)
print("Predicted sequence: ", end='')
for i in range(5):
    j = np.argmax(decoder_output[i])
    print(words[j], end='')
print()

# Training for 20 epochs
epochs = 20
for epoch in range(epochs):
    encoder_output = encoderModel.forward_constant(xe, 5)
    decoderModel.hidden = [encoder_output[-1]]
    decoder_output = decoderModel.forward_constant(xd, 5)
    loss = 0
    for i in range(5):
        j = np.argmax(yd[i])
        loss += -np.log(decoder_output[i][j])
    print('Loss at epoch', epoch+1, ':', loss)
    output_grads = []
    for i in range(5):
        output_grads.append(decoder_output[i] - yd[i])
    decoderModel.backward(output_grads)
    encoderModel.dL_dh_next = decoderModel.dL_dh_next
    decoderModel.update(1)
    decoderModel.input_bias = np.zeros((hidden_size, 1))
    decoderModel.output_bias = np.zeros((output_size, 1))
    encoderModel.backward()
    encoderModel.update(1)
    encoderModel.input_bias = np.zeros((hidden_size, 1))

encoder_output = encoderModel.forward_constant(xe, 5)
decoderModel.hidden = [encoder_output[-1]]
decoder_output = decoderModel.forward(initialState, 5)
print("Predicted sequence: ", end='')
for i in range(5):
    j = np.argmax(decoder_output[i])
    print(words[j], end='')
print()
loss = 0
for i in range(5):
    j = np.argmax(yd[i])
    loss += -np.log(decoder_output[i][j])
print('Final Loss:', loss)